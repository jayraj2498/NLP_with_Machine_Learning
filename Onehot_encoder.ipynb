{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Example text\n",
    "text = \"Tokenization is an important step in NLP.\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tokenization', 'is', 'an', 'important', 'step', 'in', 'NLP', '.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example text\n",
    "text = \"\"\"\n",
    "Natural language processing (NLP) is a subfield of artificial intelligence (AI) \n",
    "that focuses on the interaction between computers and humans through natural language. \n",
    "It involves the development of algorithms and models to understand, interpret, and \n",
    "generate human-like language.\n",
    "\n",
    "Tokenization is an important step in NLP. It involves breaking down text into individual words or tokens. \n",
    "After tokenization, lemmatization can be applied to reduce words to their base form. \n",
    "Stopwords, common words that don't carry much meaning, are often removed during preprocessing.\n",
    "\n",
    "NLP techniques, such as one-hot encoding, bag-of-words, and TF-IDF, help convert text data into a format \n",
    "that can be used by machine learning models. Word embeddings, such as Word2Vec and GloVe, provide \n",
    "dense vector representations for words, capturing semantic relationships.\n",
    "\n",
    "Understanding these techniques is crucial for working with text data and building effective NLP applications.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "tokens = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords and punctuation\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [token.lower() for token in lemmatized_tokens if token.lower() not in stop_words and token not in punctuation]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Vocabulary\n",
    "vocabulary = set(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jayraj\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "one_hot_encoded = encoder.fit_transform([[word] for word in vocabulary])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: {'nlp', 'carry', 'common', 'intelligence', 'model', 'computer', 'machine', 'learning', 'effective', 'interaction', 'individual', 'human', 'word', 'application', 'reduce', 'convert', 'subfield', 'ai', 'format', 'dense', 'removed', 'often', 'focus', 'important', 'step', 'text', 'preprocessing', 'base', 'tf-idf', 'embeddings', 'meaning', 'encoding', 'language', 'breaking', 'glove', 'understanding', 'vector', 'help', 'stopwords', \"n't\", 'bag-of-words', 'form', 'representation', 'tokenization', 'crucial', 'human-like', 'development', 'token', 'artificial', 'understand', 'technique', 'applied', 'data', 'involves', 'used', 'provide', 'lemmatization', 'algorithm', 'generate', 'capturing', 'semantic', 'relationship', 'interpret', 'building', 'natural', 'working', 'much', 'one-hot', 'word2vec', 'processing'}\n",
      "One-Hot Encoding:\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Print the vocabulary and one-hot encoding\n",
    "print(\"Vocabulary:\", vocabulary)\n",
    "print(\"One-Hot Encoding:\\n\", one_hot_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
