{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the realm of natural language processing (NLP), tokenization is a fundamental preprocessing step.\n",
      "Sentence tokenization involves breaking a paragraph into individual sentences.\n",
      "For example, consider the following text: \"NLP is fascinating.\n",
      "It involves the interaction between computers and human language.\"\n",
      "Sentence tokenization would yield two sentences.\n",
      "Next, we move on to word tokenization, where each sentence is broken down into individual words or tokens.\n",
      "Using NLTK or spaCy, we can easily tokenize these sentences.\n",
      "For instance, the sentence \"NLP is fascinating.\"\n",
      "would be tokenized into [\"NLP\", \"is\", \"fascinating\", \".\"].\n",
      "This dual process of sentence and word tokenization is a foundational step in extracting meaningful information from text data.\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize , word_tokenize\n",
    "\n",
    "paragraph = \"In the realm of natural language processing (NLP), tokenization is a fundamental preprocessing step. Sentence tokenization involves breaking a paragraph into individual sentences. For example, consider the following text: \\\"NLP is fascinating. It involves the interaction between computers and human language.\\\" Sentence tokenization would yield two sentences. Next, we move on to word tokenization, where each sentence is broken down into individual words or tokens. Using NLTK or spaCy, we can easily tokenize these sentences. For instance, the sentence \\\"NLP is fascinating.\\\" would be tokenized into [\\\"NLP\\\", \\\"is\\\", \\\"fascinating\\\", \\\".\\\"]. This dual process of sentence and word tokenization is a foundational step in extracting meaningful information from text data.\"\n",
    "\n",
    "sentences = sent_tokenize(paragraph)\n",
    "\n",
    "for sentence in sentences:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words: ['In', 'the', 'realm', 'of', 'natural', 'language', 'processing', '(', 'NLP', ')', ',', 'tokenization', 'is', 'a', 'fundamental', 'preprocessing', 'step', '.']\n",
      "Words: ['Sentence', 'tokenization', 'involves', 'breaking', 'a', 'paragraph', 'into', 'individual', 'sentences', '.']\n",
      "Words: ['For', 'example', ',', 'consider', 'the', 'following', 'text', ':', '``', 'NLP', 'is', 'fascinating', '.']\n",
      "Words: ['It', 'involves', 'the', 'interaction', 'between', 'computers', 'and', 'human', 'language', '.', \"''\"]\n",
      "Words: ['Sentence', 'tokenization', 'would', 'yield', 'two', 'sentences', '.']\n",
      "Words: ['Next', ',', 'we', 'move', 'on', 'to', 'word', 'tokenization', ',', 'where', 'each', 'sentence', 'is', 'broken', 'down', 'into', 'individual', 'words', 'or', 'tokens', '.']\n",
      "Words: ['Using', 'NLTK', 'or', 'spaCy', ',', 'we', 'can', 'easily', 'tokenize', 'these', 'sentences', '.']\n",
      "Words: ['For', 'instance', ',', 'the', 'sentence', '``', 'NLP', 'is', 'fascinating', '.', \"''\"]\n",
      "Words: ['would', 'be', 'tokenized', 'into', '[', '``', 'NLP', \"''\", ',', '``', 'is', \"''\", ',', '``', 'fascinating', \"''\", ',', '``', '.', '``', ']', '.']\n",
      "Words: ['This', 'dual', 'process', 'of', 'sentence', 'and', 'word', 'tokenization', 'is', 'a', 'foundational', 'step', 'in', 'extracting', 'meaningful', 'information', 'from', 'text', 'data', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "paragraph = \"In the realm of natural language processing (NLP), tokenization is a fundamental preprocessing step. Sentence tokenization involves breaking a paragraph into individual sentences. For example, consider the following text: \\\"NLP is fascinating. It involves the interaction between computers and human language.\\\" Sentence tokenization would yield two sentences. Next, we move on to word tokenization, where each sentence is broken down into individual words or tokens. Using NLTK or spaCy, we can easily tokenize these sentences. For instance, the sentence \\\"NLP is fascinating.\\\" would be tokenized into [\\\"NLP\\\", \\\"is\\\", \\\"fascinating\\\", \\\".\\\"]. This dual process of sentence and word tokenization is a foundational step in extracting meaningful information from text data.\"\n",
    "\n",
    "sentences = sent_tokenize(paragraph)\n",
    "\n",
    "for sentence in sentences:\n",
    "    words = word_tokenize(sentence)\n",
    "    print(f\"Words: {words}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# paragraph / corpus to sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph ='''Topic sentences are similar to mini thesis statements. \n",
    "    Like a thesis statement, a topic sentence has a specific main point.\n",
    "    Whereas the thesis is the main point of the essay, the topic sentence is the main point of the paragraph.\n",
    "    Like the thesis statement, a topic sentence has a unifying function.\n",
    "    But a thesis statement or topic sentence alone does not  guarantee unity. \n",
    "    An essay is unified if all the paragraphs relate to the thesis,\n",
    "    whereas a paragraph is unified if all the sentences relate to the topic sentence.\n",
    "    Note: Not all paragraphs need topic sentences. In particular, \n",
    "    opening and closing paragraphs, which serve different functions from body paragraphs,\n",
    "    generally do not have topic sentence'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic sentences are similar to mini thesis statements.\n",
      "Like a thesis statement, a topic sentence has a specific main point.\n",
      "Whereas the thesis is the main point of the essay, the topic sentence is the main point of the paragraph.\n",
      "Like the thesis statement, a topic sentence has a unifying function.\n",
      "But a thesis statement or topic sentence alone does not  guarantee unity.\n",
      "An essay is unified if all the paragraphs relate to the thesis,\n",
      "    whereas a paragraph is unified if all the sentences relate to the topic sentence.\n",
      "Note: Not all paragraphs need topic sentences.\n",
      "In particular, \n",
      "    opening and closing paragraphs, which serve different functions from body paragraphs,\n",
      "    generally do not have topic sentence\n"
     ]
    }
   ],
   "source": [
    "sentences =sent_tokenize(paragraph) \n",
    "\n",
    "for sentence in sentences:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sentence to words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words: ['Topic', 'sentences', 'are', 'similar', 'to', 'mini', 'thesis', 'statements', '.', 'Like', 'a', 'thesis', 'statement', ',', 'a', 'topic', 'sentence', 'has', 'a', 'specific', 'main', 'point', '.', 'Whereas', 'the', 'thesis', 'is', 'the', 'main', 'point', 'of', 'the', 'essay', ',', 'the', 'topic', 'sentence', 'is', 'the', 'main', 'point', 'of', 'the', 'paragraph', '.', 'Like', 'the', 'thesis', 'statement', ',', 'a', 'topic', 'sentence', 'has', 'a', 'unifying', 'function', '.', 'But', 'a', 'thesis', 'statement', 'or', 'topic', 'sentence', 'alone', 'does', 'not', 'guarantee', 'unity', '.', 'An', 'essay', 'is', 'unified', 'if', 'all', 'the', 'paragraphs', 'relate', 'to', 'the', 'thesis', ',', 'whereas', 'a', 'paragraph', 'is', 'unified', 'if', 'all', 'the', 'sentences', 'relate', 'to', 'the', 'topic', 'sentence', '.', 'Note', ':', 'Not', 'all', 'paragraphs', 'need', 'topic', 'sentences', '.', 'In', 'particular', ',', 'opening', 'and', 'closing', 'paragraphs', ',', 'which', 'serve', 'different', 'functions', 'from', 'body', 'paragraphs', ',', 'generally', 'do', 'not', 'have', 'topic', 'sentence']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "paragraph = '''Topic sentences are similar to mini thesis statements. \n",
    "    Like a thesis statement, a topic sentence has a specific main point.\n",
    "    Whereas the thesis is the main point of the essay, the topic sentence is the main point of the paragraph.\n",
    "    Like the thesis statement, a topic sentence has a unifying function.\n",
    "    But a thesis statement or topic sentence alone does not guarantee unity. \n",
    "    An essay is unified if all the paragraphs relate to the thesis,\n",
    "    whereas a paragraph is unified if all the sentences relate to the topic sentence.\n",
    "    Note: Not all paragraphs need topic sentences. In particular, \n",
    "    opening and closing paragraphs, which serve different functions from body paragraphs,\n",
    "    generally do not have topic sentence'''\n",
    "\n",
    "words = word_tokenize(paragraph) \n",
    "\n",
    "print(f\"Words: {words}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words: Topic\n",
      "Words: sentences\n",
      "Words: are\n",
      "Words: similar\n",
      "Words: to\n",
      "Words: mini\n",
      "Words: thesis\n",
      "Words: statements\n",
      "Words: .\n",
      "Words: Like\n",
      "Words: a\n",
      "Words: thesis\n",
      "Words: statement\n",
      "Words: ,\n",
      "Words: a\n",
      "Words: topic\n",
      "Words: sentence\n",
      "Words: has\n",
      "Words: a\n",
      "Words: specific\n",
      "Words: main\n",
      "Words: point\n",
      "Words: .\n",
      "Words: Whereas\n",
      "Words: the\n",
      "Words: thesis\n",
      "Words: is\n",
      "Words: the\n",
      "Words: main\n",
      "Words: point\n",
      "Words: of\n",
      "Words: the\n",
      "Words: essay\n",
      "Words: ,\n",
      "Words: the\n",
      "Words: topic\n",
      "Words: sentence\n",
      "Words: is\n",
      "Words: the\n",
      "Words: main\n",
      "Words: point\n",
      "Words: of\n",
      "Words: the\n",
      "Words: paragraph\n",
      "Words: .\n",
      "Words: Like\n",
      "Words: the\n",
      "Words: thesis\n",
      "Words: statement\n",
      "Words: ,\n",
      "Words: a\n",
      "Words: topic\n",
      "Words: sentence\n",
      "Words: has\n",
      "Words: a\n",
      "Words: unifying\n",
      "Words: function\n",
      "Words: .\n",
      "Words: But\n",
      "Words: a\n",
      "Words: thesis\n",
      "Words: statement\n",
      "Words: or\n",
      "Words: topic\n",
      "Words: sentence\n",
      "Words: alone\n",
      "Words: does\n",
      "Words: not\n",
      "Words: guarantee\n",
      "Words: unity\n",
      "Words: .\n",
      "Words: An\n",
      "Words: essay\n",
      "Words: is\n",
      "Words: unified\n",
      "Words: if\n",
      "Words: all\n",
      "Words: the\n",
      "Words: paragraphs\n",
      "Words: relate\n",
      "Words: to\n",
      "Words: the\n",
      "Words: thesis\n",
      "Words: ,\n",
      "Words: whereas\n",
      "Words: a\n",
      "Words: paragraph\n",
      "Words: is\n",
      "Words: unified\n",
      "Words: if\n",
      "Words: all\n",
      "Words: the\n",
      "Words: sentences\n",
      "Words: relate\n",
      "Words: to\n",
      "Words: the\n",
      "Words: topic\n",
      "Words: sentence\n",
      "Words: .\n",
      "Words: Note\n",
      "Words: :\n",
      "Words: Not\n",
      "Words: all\n",
      "Words: paragraphs\n",
      "Words: need\n",
      "Words: topic\n",
      "Words: sentences\n",
      "Words: .\n",
      "Words: In\n",
      "Words: particular\n",
      "Words: ,\n",
      "Words: opening\n",
      "Words: and\n",
      "Words: closing\n",
      "Words: paragraphs\n",
      "Words: ,\n",
      "Words: which\n",
      "Words: serve\n",
      "Words: different\n",
      "Words: functions\n",
      "Words: from\n",
      "Words: body\n",
      "Words: paragraphs\n",
      "Words: ,\n",
      "Words: generally\n",
      "Words: do\n",
      "Words: not\n",
      "Words: have\n",
      "Words: topic\n",
      "Words: sentence\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "paragraph = '''Topic sentences are similar to mini thesis statements. \n",
    "    Like a thesis statement, a topic sentence has a specific main point.\n",
    "    Whereas the thesis is the main point of the essay, the topic sentence is the main point of the paragraph.\n",
    "    Like the thesis statement, a topic sentence has a unifying function.\n",
    "    But a thesis statement or topic sentence alone does not guarantee unity. \n",
    "    An essay is unified if all the paragraphs relate to the thesis,\n",
    "    whereas a paragraph is unified if all the sentences relate to the topic sentence.\n",
    "    Note: Not all paragraphs need topic sentences. In particular, \n",
    "    opening and closing paragraphs, which serve different functions from body paragraphs,\n",
    "    generally do not have topic sentence'''\n",
    "\n",
    "words = word_tokenize(paragraph) \n",
    "\n",
    "for i in words :\n",
    "\n",
    "    print(f\"Words: {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "### paragraph to sentences to words tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences:\n",
      "Topic sentences are similar to mini thesis statements.\n",
      "Like a thesis statement, a topic sentence has a specific main point.\n",
      "Whereas the thesis is the main point of the essay, the topic sentence is the main point of the paragraph.\n",
      "Like the thesis statement, a topic sentence has a unifying function.\n",
      "But a thesis statement or topic sentence alone does not guarantee unity.\n",
      "An essay is unified if all the paragraphs relate to the thesis,\n",
      "    whereas a paragraph is unified if all the sentences relate to the topic sentence.\n",
      "Note: Not all paragraphs need topic sentences.\n",
      "In particular, \n",
      "    opening and closing paragraphs, which serve different functions from body paragraphs,\n",
      "    generally do not have topic sentence\n",
      "\n",
      "Word Tokenization:\n",
      "Words: ['Topic', 'sentences', 'are', 'similar', 'to', 'mini', 'thesis', 'statements', '.']\n",
      "Words: ['Like', 'a', 'thesis', 'statement', ',', 'a', 'topic', 'sentence', 'has', 'a', 'specific', 'main', 'point', '.']\n",
      "Words: ['Whereas', 'the', 'thesis', 'is', 'the', 'main', 'point', 'of', 'the', 'essay', ',', 'the', 'topic', 'sentence', 'is', 'the', 'main', 'point', 'of', 'the', 'paragraph', '.']\n",
      "Words: ['Like', 'the', 'thesis', 'statement', ',', 'a', 'topic', 'sentence', 'has', 'a', 'unifying', 'function', '.']\n",
      "Words: ['But', 'a', 'thesis', 'statement', 'or', 'topic', 'sentence', 'alone', 'does', 'not', 'guarantee', 'unity', '.']\n",
      "Words: ['An', 'essay', 'is', 'unified', 'if', 'all', 'the', 'paragraphs', 'relate', 'to', 'the', 'thesis', ',', 'whereas', 'a', 'paragraph', 'is', 'unified', 'if', 'all', 'the', 'sentences', 'relate', 'to', 'the', 'topic', 'sentence', '.']\n",
      "Words: ['Note', ':', 'Not', 'all', 'paragraphs', 'need', 'topic', 'sentences', '.']\n",
      "Words: ['In', 'particular', ',', 'opening', 'and', 'closing', 'paragraphs', ',', 'which', 'serve', 'different', 'functions', 'from', 'body', 'paragraphs', ',', 'generally', 'do', 'not', 'have', 'topic', 'sentence']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "paragraph = '''Topic sentences are similar to mini thesis statements. \n",
    "    Like a thesis statement, a topic sentence has a specific main point.\n",
    "    Whereas the thesis is the main point of the essay, the topic sentence is the main point of the paragraph.\n",
    "    Like the thesis statement, a topic sentence has a unifying function.\n",
    "    But a thesis statement or topic sentence alone does not guarantee unity. \n",
    "    An essay is unified if all the paragraphs relate to the thesis,\n",
    "    whereas a paragraph is unified if all the sentences relate to the topic sentence.\n",
    "    Note: Not all paragraphs need topic sentences. In particular, \n",
    "    opening and closing paragraphs, which serve different functions from body paragraphs,\n",
    "    generally do not have topic sentence'''\n",
    "\n",
    "# Sentence Tokenization\n",
    "sentences = sent_tokenize(paragraph)\n",
    "print(\"Sentences:\")\n",
    "for sentence in sentences:\n",
    "    print(sentence)\n",
    "\n",
    "# Word Tokenization for each sentence\n",
    "print(\"\\nWord Tokenization:\")\n",
    "for sentence in sentences:\n",
    "    words = word_tokenize(sentence)\n",
    "    print(f\"Words: {words}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Custom', 'tokenization', '#example', 'handle', 'special', 'cases']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"Custom tokenization #example, handle special cases.\"\n",
    "tokens = re.findall(r'\\w+|#\\w+|@\\w+', text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=\"\"\"Hello Welcome, NLP Tutorials.\n",
    "Please do watch the entire course! to become expert in NLP.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Tokenization\n",
    "## Sentence-->paragraphs\n",
    "from nltk.tokenize import sent_tokenize\n",
    "documents=sent_tokenize(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Welcome, NLP Tutorials.\n",
      "Please do watch the entire course!\n",
      "to become expert in NLP.\n"
     ]
    }
   ],
   "source": [
    "for sentence in documents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenization \n",
    "## Paragraph-->words\n",
    "## sentence--->words\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Welcome',\n",
       " ',',\n",
       " 'NLP',\n",
       " 'Tutorials',\n",
       " '.',\n",
       " 'Please',\n",
       " 'do',\n",
       " 'watch',\n",
       " 'the',\n",
       " 'entire',\n",
       " 'course',\n",
       " '!',\n",
       " 'to',\n",
       " 'become',\n",
       " 'expert',\n",
       " 'in',\n",
       " 'NLP',\n",
       " '.']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Welcome',\n",
       " ',',\n",
       " 'NLP',\n",
       " 'Tutorials',\n",
       " '.',\n",
       " 'Please',\n",
       " 'do',\n",
       " 'watch',\n",
       " 'the',\n",
       " 'entire',\n",
       " 'course',\n",
       " '!',\n",
       " 'to',\n",
       " 'become',\n",
       " 'expert',\n",
       " 'in',\n",
       " 'NLP',\n",
       " '.']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "wordpunct_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
